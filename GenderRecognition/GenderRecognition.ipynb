{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import math\n",
    "from scipy.ndimage import filters\n",
    "from numpy import floor\n",
    "from math import atan\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def show_image(img):\n",
    "    cv2.imshow('image',img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weber local descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Magic numbers\n",
    "M = 4\n",
    "T = 12\n",
    "S = 4\n",
    "\n",
    "eps = 0.000001\n",
    "\n",
    "def ratio(x):\n",
    "    # x[4] - центральный элемент\n",
    "    value = 0.0\n",
    "    if x[4] == 0:\n",
    "        value = pi / 2.0\n",
    "    else:\n",
    "        value = atan(float(x.sum() - 9.0 * x[4]) / x[4])\n",
    "    return value\n",
    "\n",
    "def intense_fraction(x):\n",
    "    value = 0.0\n",
    "    if x[5] == x[1]:\n",
    "        value = 2.0 * pi\n",
    "    else:\n",
    "        value = 2.0 * atan(float(x[7] - x[3]) / (x[5] - x[1])) + pi\n",
    "    return value\n",
    "\n",
    "def get_features(image):\n",
    "    image = [[float(j) for j in i] for i in image]\n",
    "    # Calculating differential excitation\n",
    "    image_de = filters.generic_filter(image, ratio, (3, 3))\n",
    "    # Calculating gradient orientation\n",
    "    image_teta = filters.generic_filter(image, intense_fraction, (3, 3))\n",
    "    # Calculating dominant orientation\n",
    "    phi = 2.0 * pi * (floor((image_teta * T) / (2.0 * pi) + 1.0 / 2.0) % T) / T\n",
    "    # Each phi[j] is from T values: {phi_0, phi_1, ....phi_(T-1)}\n",
    "    H_t = []\n",
    "    for t in range(T):\n",
    "        phi_t = 2.0 * pi * t / T \n",
    "        # Indicies x and y coordinates   \n",
    "        indicies = np.where(abs(phi - phi_t) < eps)\n",
    "        # We wil build histogram on diff excitiation\n",
    "        values = image_de[indicies[0], indicies[1]]\n",
    "        # In H_t (array_of_values, array_of_coords)\n",
    "        hist, bins = np.histogram(values, bins=M * S)\n",
    "        H_t.append(hist)\n",
    "        # width = 0.7 * (bins[1] - bins[0])\n",
    "        # center = (bins[:-1] + bins[1:]) / 2\n",
    "        # plt.bar(center, hist, align='center', width=width)\n",
    "        # plt.show()\n",
    "    \n",
    "    # Divide each of t histograms to m parts\n",
    "    length = len(H_t[0])\n",
    "    # print \"length = \" + str(length)\n",
    "    # print \"first hist:\"\n",
    "    # print H_t[0]\n",
    "    # -----> t\n",
    "    # |\n",
    "    # |\n",
    "    # |\n",
    "    # |\n",
    "    # v m\n",
    "    H_tm = []\n",
    "    for t in range(T):\n",
    "        hist_part = []\n",
    "        for m in range(M):\n",
    "            hist_part.append(H_t[t][m * length / M: (m + 1) * length / M])\n",
    "        H_tm.append(hist_part)\n",
    "\n",
    "    # H_tm = [hist_part_1, ..., hist_part_T],\n",
    "    # where hist_part_t = [hist_1, .., hist_M]\n",
    "    H_m = []\n",
    "    for m in range(M):\n",
    "        hist_m = []\n",
    "        for t in range(T):\n",
    "            hist_m += [value for value in H_tm[t][m]]\n",
    "        H_m.append(hist_m)\n",
    "\n",
    "    H = []\n",
    "    for i in range(M):\n",
    "        H += H_m[i]\n",
    "        \n",
    "    # N = len(H)\n",
    "    #x = range(N)\n",
    "    #width = 1/1.5\n",
    "    #plt.bar(x, H, width, color=\"blue\")\n",
    "    #plt.show()\n",
    "    return H\n",
    "\n",
    "def WLD(image, num_of_blocks=9):\n",
    "    height = len(image)\n",
    "    width = len(image[0])\n",
    "    features = []\n",
    "    if num_of_blocks == 4:\n",
    "        features += get_features(image[0:height/2, 0:width/2])\n",
    "        features += get_features(image[0:height/2, width/2:width])\n",
    "        features += get_features(image[height/2:height, 0:width/2])\n",
    "        features += get_features(image[height/2:height, width/2:width])\n",
    "        return features\n",
    "    elif num_of_blocks == 9:\n",
    "        x_1_3 = width * 1 / 3\n",
    "        x_2_3 = width * 2 / 3\n",
    "        y_1_3 = height * 1 / 3\n",
    "        y_2_3 = height * 2 / 3\n",
    "        features += get_features(image[0:y_1_3,0:x_1_3])\n",
    "        features += get_features(image[y_1_3:y_2_3,0:x_1_3])\n",
    "        features += get_features(image[y_2_3:height,0:x_1_3])\n",
    "        features += get_features(image[0:y_1_3,x_1_3:x_2_3])\n",
    "        features += get_features(image[y_1_3:y_2_3,x_1_3:x_2_3])\n",
    "        features += get_features(image[y_2_3:height,x_1_3:x_2_3])\n",
    "        features += get_features(image[0:y_1_3,x_2_3:width])\n",
    "        features += get_features(image[y_1_3:y_2_3,x_2_3:width])\n",
    "        features += get_features(image[y_2_3:height,x_2_3:width])   \n",
    "        return features\n",
    "    elif num_of_blocks == 1:\n",
    "        features += get_features(image)\n",
    "        return features\n",
    "    else:\n",
    "        print str(num_of_blocks) + \" : Number of blocks not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Обучаемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female pictures:\n",
      "9336923\n",
      "9338535\n",
      "9540814\n",
      "9540849\n",
      "amflem\n",
      "anpage\n",
      "aolcer\n",
      "apapou\n",
      "asamma\n",
      "asewil\n",
      "astefa\n",
      "boylee\n",
      "drbost\n",
      "ekavaz\n",
      "elduns\n",
      "gghazv\n",
      "gotone\n",
      "hsgrim\n",
      "isbald\n",
      "jbierl\n",
      "jross\n",
      "kaknig\n",
      "klclar\n",
      "ksunth\n",
      "labenm\n",
      "lcelli\n",
      "ldgodd\n",
      "lfso\n",
      "mbutle\n",
      "mclarkd\n",
      "mkosto\n",
      "mkotza\n",
      "phughe\n",
      "rhosan\n",
      "sbains\n",
      "shpill\n",
      "slbirc\n",
      "smfarrf\n",
      "vanta\n",
      "vstros\n",
      "wylsow\n",
      "yfhsie\n"
     ]
    }
   ],
   "source": [
    "# STUB: пока что только фронтальный лица\n",
    "X_train = []\n",
    "labels_train = []\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "female_train_path = os.path.abspath(\"train_dataset/female\")\n",
    "male_train_path = os.path.abspath(\"train_dataset/male\")\n",
    "\n",
    "# FEMALE\n",
    "print \"Female pictures:\"\n",
    "for filename in os.listdir(female_train_path):\n",
    "    print filename\n",
    "    for picture in os.listdir(os.path.join(female_train_path, filename)):\n",
    "         if picture.endswith(\".jpg\"): \n",
    "            picture_path = os.path.abspath(os.path.join(os.path.join(female_train_path, filename), picture))\n",
    "            img = cv2.imread(picture_path)\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "            for (x, y, w, h) in faces:\n",
    "                x_stop = x + w\n",
    "                y_stop = y + h\n",
    "                # rect = cv2.rectangle(img,(x,y),(x_stop,y_stop),(0,255,0),2)\n",
    "                X_train.append(WLD(gray_img[y:y_stop, x:x_stop]))\n",
    "                labels_train.append(0)\n",
    "                # Здесь одно лицо на каждом образце\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male pictures:\n",
      "9326871\n",
      "9332898\n",
      "9338446\n",
      "9338454\n",
      "9338462\n",
      "9338489\n",
      "9338497\n",
      "9338519\n",
      "9338527\n",
      "9338543\n",
      "9414649\n",
      "9416994\n",
      "admars\n",
      "ahodki\n",
      "ajflem\n",
      "ajones\n",
      "ajsega\n",
      "akatsi\n",
      "ambarw\n",
      "anonym\n",
      "anonym1\n",
      "anonym2\n",
      "asheal\n",
      "bplyce\n",
      "cchris\n",
      "ccjame\n",
      "cferdo\n",
      "cgboyc\n",
      "cjcarr\n",
      "cjdenn\n",
      "cjsake\n",
      "cmkirk\n",
      "csanch\n",
      "cshubb\n",
      "cwang\n",
      "cwchoi\n",
      "dagran\n",
      "dakram\n",
      "dcbowe\n",
      "dioann\n",
      "djbirc\n",
      "djhugh\n",
      "djmart\n",
      "dmwest\n",
      "doraj\n",
      "fordj\n",
      "gdhatc\n",
      "ggeorg\n",
      "ggrego\n",
      "gjhero\n",
      "gjnorm\n",
      "gmwate\n",
      "gpapaz\n",
      "gpsmit\n",
      "gsreas\n",
      "hartb\n",
      "hensm\n",
      "ieorf\n",
      "irdrew\n",
      "jabins\n",
      "jagrif\n",
      "jcarte\n",
      "jdbenm\n",
      "jgloma\n",
      "jlemon\n",
      "jmedin\n",
      "jrtobi\n",
      "kaatki\n",
      "kdjone\n",
      "khchan\n",
      "khughe\n",
      "kjwith\n",
      "lejnno\n",
      "lyond\n",
      "maasht\n",
      "macci\n",
      "martin\n",
      "mberdo\n",
      "mdpove\n",
      "mefait\n",
      "mhwill\n",
      "miaduc\n",
      "michael\n",
      "mjhans\n",
      "moors\n",
      "mpetti\n",
      "muthay\n",
      "nahaig\n",
      "namull\n",
      "ndbank\n",
      "ndhagu\n",
      "nhrams\n",
      "njmoor\n",
      "npbour\n",
      "npmitc\n",
      "nrclar\n",
      "nrrbar\n",
      "nwilli\n",
      "obeidn\n",
      "ohpark\n",
      "pacole\n",
      "pmives\n",
      "pshurr\n",
      "pspliu\n",
      "ptnich\n",
      "rarobi\n",
      "rgharr\n",
      "rgspru\n",
      "rjlabr\n",
      "rlocke\n",
      "rmcoll\n",
      "rmpugh\n",
      "rnpwil\n",
      "robin\n",
      "rrowle\n",
      "rsanti\n",
      "saduah\n",
      "saedwa\n",
      "sandm\n",
      "sidick\n",
      "sjbeck\n",
      "skumar\n",
      "smrobb\n",
      "spacl\n",
      "spletc\n",
      "svkriz\n",
      "swewin\n",
      "swsmit\n",
      "tony\n",
      "voudcx\n",
      "vpsavo\n",
      "whussa\n",
      "wjalbe\n"
     ]
    }
   ],
   "source": [
    "# MALE\n",
    "print \"Male pictures:\"\n",
    "for filename in os.listdir(male_train_path):\n",
    "    print filename\n",
    "    for picture in os.listdir(os.path.join(male_train_path, filename)):\n",
    "         if picture.endswith(\".jpg\"): \n",
    "            picture_path = os.path.abspath(os.path.join(os.path.join(male_train_path, filename), picture))\n",
    "            img = cv2.imread(picture_path)\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "            for (x, y, w, h) in faces:\n",
    "                x_stop = x + w\n",
    "                y_stop = y + h\n",
    "                # rect = cv2.rectangle(img,(x,y),(x_stop,y_stop),(0,255,0),2)\n",
    "                X_train.append(WLD(gray_img[y:y_stop, x:x_stop]))\n",
    "                labels_train.append(1)\n",
    "                # Здесь одно лицо на каждом образце\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female grimaces:\n",
      "chr\n",
      "sar\n"
     ]
    }
   ],
   "source": [
    "female_grimace_path = \"grimace/female\"\n",
    "# FEMALE\n",
    "print \"Female grimaces:\"\n",
    "for filename in os.listdir(female_grimace_path):\n",
    "    print filename\n",
    "    for picture in os.listdir(os.path.join(female_grimace_path, filename)):\n",
    "         if picture.endswith(\".jpg\"): \n",
    "            picture_path = os.path.abspath(os.path.join(os.path.join(female_grimace_path, filename), picture))\n",
    "            img = cv2.imread(picture_path)\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "            for (x, y, w, h) in faces:\n",
    "                x_stop = x + w\n",
    "                y_stop = y + h\n",
    "                # rect = cv2.rectangle(img,(x,y),(x_stop,y_stop),(0,255,0),2)\n",
    "                X_train.append(WLD(gray_img[y:y_stop, x:x_stop]))\n",
    "                labels_train.append(0)\n",
    "                # Здесь одно лицо на каждом образце\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male grimaces:\n",
      "and\n",
      "ant\n",
      "dah\n",
      "dav\n",
      "den\n",
      "glen\n",
      "ian\n",
      "jer\n",
      "john\n",
      "lib\n",
      "mike\n",
      "pat\n",
      "ste\n",
      "stu\n",
      "tom\n",
      "will\n"
     ]
    }
   ],
   "source": [
    "# MALE\n",
    "male_grimace_path = \"grimace/male\"\n",
    "print \"Male grimaces:\"\n",
    "for filename in os.listdir(male_grimace_path):\n",
    "    print filename\n",
    "    for picture in os.listdir(os.path.join(male_grimace_path, filename)):\n",
    "         if picture.endswith(\".jpg\"): \n",
    "            picture_path = os.path.abspath(os.path.join(os.path.join(male_grimace_path, filename), picture))\n",
    "            img = cv2.imread(picture_path)\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray_img, 1.3, 5)\n",
    "            for (x, y, w, h) in faces:\n",
    "                x_stop = x + w\n",
    "                y_stop = y + h\n",
    "                # rect = cv2.rectangle(img,(x,y),(x_stop,y_stop),(0,255,0),2)\n",
    "                X_train.append(WLD(gray_img[y:y_stop, x:x_stop]))\n",
    "                labels_train.append(1)\n",
    "                # Здесь одно лицо на каждом образце\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And data is:\n",
      "3717\n",
      "3717\n"
     ]
    }
   ],
   "source": [
    "print \"And data is:\"\n",
    "print len(X_train)\n",
    "print len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# На тестовой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Парсим тестовую базу\n",
    "import csv\n",
    "files_to_X = dict()\n",
    "with open('test_dataset//markup.csv', 'rb') as csvfile:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    for raw in data:\n",
    "        file_name = raw[0].strip() + \".tiff\"\n",
    "        if file_name not in files_to_X:\n",
    "            files_to_X[file_name] = list()\n",
    "        files_to_X[file_name].append([raw[1].strip(), raw[2].strip(), raw[3].strip(), raw[4].strip(), raw[5].strip()])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000009.tiff\n",
      "0000030.tiff\n",
      "0000033.tiff\n",
      "0000013.tiff\n",
      "0000049.tiff\n",
      "0000048.tiff\n",
      "0000050.tiff\n",
      "0000029.tiff\n",
      "0000031.tiff\n",
      "0000027.tiff\n",
      "0000010.tiff\n",
      "0000053.tiff\n",
      "0000028.tiff\n",
      "0000046.tiff\n",
      "0000012.tiff\n",
      "0000011.tiff\n",
      "0000032.tiff\n",
      "0000044.tiff\n",
      "0000023.tiff\n",
      "0000047.tiff\n",
      "0000051.tiff\n"
     ]
    }
   ],
   "source": [
    "# Проверяем на ней\n",
    "X = []\n",
    "labels = []\n",
    "for file, faces in files_to_X.iteritems():\n",
    "    print file\n",
    "    img = cv2.imread(\"test_dataset\\\\\" + file)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    for face in faces:\n",
    "        x = int(face[0])\n",
    "        y = int(face[1])\n",
    "        w = int(face[2]) - x\n",
    "        h = int(face[3]) - y\n",
    "        x_stop = x + w\n",
    "        y_stop = y + h\n",
    "        if face[4] == 'f':\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "        X.append(WLD(gray_img[y:y_stop, x:x_stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     female       0.70      0.38      0.49       110\n",
      "       male       0.48      0.78      0.60        82\n",
      "\n",
      "avg / total       0.61      0.55      0.54       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nbrs = KNeighborsClassifier(n_neighbors=4, metric='canberra')\n",
    "nbrs.fit(X_train, labels_train)\n",
    "predicted = nbrs.predict(X)\n",
    "target_names = ['female', 'male']\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(labels, predicted, target_names=target_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Print in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000009.tiff\n",
      "0000030.tiff\n",
      "0000033.tiff\n",
      "0000013.tiff\n",
      "0000049.tiff\n",
      "0000048.tiff\n",
      "0000050.tiff\n",
      "0000029.tiff\n",
      "0000031.tiff\n",
      "0000027.tiff\n",
      "0000010.tiff\n",
      "0000053.tiff\n",
      "0000028.tiff\n",
      "0000046.tiff\n",
      "0000012.tiff\n",
      "0000011.tiff\n",
      "0000032.tiff\n",
      "0000044.tiff\n",
      "0000023.tiff\n",
      "0000047.tiff\n",
      "0000051.tiff\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for file, faces in files_to_X.iteritems():\n",
    "    print file\n",
    "    img = cv2.imread(\"test_dataset\\\\\" + file)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    for face in faces:\n",
    "        x = int(face[0])\n",
    "        y = int(face[1])\n",
    "        x_stop = int(face[2])\n",
    "        y_stop = int(face[3])\n",
    "        sex = ''\n",
    "        if predicted[counter] == 1:\n",
    "            sex = 'm'\n",
    "        else:\n",
    "            sex = 'f'\n",
    "        with open('output.csv', 'ab') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "            row = [file[:-5], x, y, x_stop, y_stop, sex]\n",
    "            writer.writerow(row)\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIKI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('wiki/wiki.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiki_format = {\n",
    "    'dob': 0,\n",
    "    'photo_taken': 1,\n",
    "    'full_path': 2,\n",
    "    'gender': 3,\n",
    "    'name': 4,\n",
    "    'face_location': 5,\n",
    "    'face_score': 6,\n",
    "    'second_face_score': 7\n",
    "}\n",
    "\n",
    "genders = []\n",
    "face_location = []\n",
    "file_path = []\n",
    "face_score = []\n",
    "second_face_score = []\n",
    "\n",
    "wiki_dataset = mat['wiki']\n",
    "for i in range(1):\n",
    "    for j in range(1):\n",
    "        for k in range(8):\n",
    "            if wiki_format['gender'] == k:\n",
    "                genders = wiki_dataset[i][j][k][0]\n",
    "            elif wiki_format['full_path'] == k:\n",
    "                file_path = wiki_dataset[i][j][k][0]\n",
    "            elif wiki_format['face_location'] == k:\n",
    "                face_location = wiki_dataset[i][j][k][0]\n",
    "            elif wiki_format['face_score'] == k:\n",
    "                face_score = wiki_dataset[i][j][k][0]\n",
    "            elif wiki_format['second_face_score'] == k:\n",
    "                second_face_score = wiki_dataset[i][j][k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def image_fits(gender, path, score1, score2, face_coords):\n",
    "    # print gender, path, score1, score2, face_coords\n",
    "    fits = True\n",
    "    fits = (fits) and (path != \"\")\n",
    "    fits = (fits) and (not math.isnan(gender)) and ((int(gender) == 0) or (int(gender) == 1))\n",
    "    fits = (fits) and (len(face_coords) == 4)\n",
    "    fits = (fits) and (not math.isinf(score1))\n",
    "    fits = (fits) and (math.isnan(score2))\n",
    "    return fits\n",
    "\n",
    "N = len(file_path)\n",
    "print N\n",
    "X_wiki = []\n",
    "labels_wiki = []\n",
    "for i in range(N):\n",
    "    if i == 10000:\n",
    "        break\n",
    "    if image_fits(genders[i], file_path[i], face_score[i], second_face_score[i], face_location[i][0]):\n",
    "        path = os.path.abspath(os.path.join(\"wiki\", file_path[i][0]))\n",
    "        img = cv2.imread(path, 0)\n",
    "        if img == None:\n",
    "            continue\n",
    "        # gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        x = int(face_location[i][0][0])\n",
    "        y = int(face_location[i][0][1])\n",
    "        x_stop = int(face_location[i][0][2])\n",
    "        y_stop = int(face_location[i][0][3])\n",
    "        # rect = cv2.rectangle(img,(x,y),(x_stop,y_stop),(0,255,0),2)\n",
    "        # show_image(rect)\n",
    "        q = WLD(img[y:y_stop, x:x_stop])\n",
    "        if np.isnan(q).any():\n",
    "            continue\n",
    "        labels_wiki.append(int(genders[i]))\n",
    "        X_wiki.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(X_wiki)\n",
    "print len(labels_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверяем с WIKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_wiki = RandomForestClassifier(n_estimators=100)\n",
    "model_wiki.fit(X_wiki, labels_wiki)\n",
    "predicted = model_wiki.predict(X)\n",
    "target_names = ['female', 'male']\n",
    "print classification_report(labels, predicted, target_names=target_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbrs = KNeighborsClassifier(n_neighbors=5, metric='canberra')\n",
    "nbrs.fit(X_wiki, labels_wiki)\n",
    "predicted = nbrs.predict(X)\n",
    "target_names = ['female', 'male']\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(labels, predicted, target_names=target_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
